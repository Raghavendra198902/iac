# Production Docker Compose Configuration
# For production deployment with security hardening and high availability

version: '3.8'

x-common-environment: &common-env
  NODE_ENV: production
  LOG_LEVEL: ${LOG_LEVEL:-info}
  LOG_FORMAT: json
  ENABLE_METRICS: "true"
  ENABLE_TRACING: "true"

x-common-security: &common-security
  security_opt:
    - no-new-privileges:true
  cap_drop:
    - ALL
  cap_add:
    - NET_BIND_SERVICE
  read_only: true
  tmpfs:
    - /tmp:noexec,nosuid,size=128M

x-common-health: &common-health
  interval: 30s
  timeout: 10s
  retries: 3
  start_period: 40s

services:
  # ═══════════════════════════════════════════════════════════
  # DATABASE TIER
  # ═══════════════════════════════════════════════════════════
  postgres:
    image: postgres:15-alpine
    container_name: dharma-postgres-prod
    restart: unless-stopped
    <<: *common-security
    environment:
      POSTGRES_DB: ${DB_NAME:-iac_dharma_prod}
      POSTGRES_USER: ${DB_USER:-dharma_prod}
      POSTGRES_PASSWORD: ${DB_PASSWORD:?Database password required}
      POSTGRES_INITDB_ARGS: "--encoding=UTF8 --lc-collate=en_US.UTF-8 --lc-ctype=en_US.UTF-8"
      PGDATA: /var/lib/postgresql/data/pgdata
    ports:
      - "127.0.0.1:5432:5432"
    volumes:
      - postgres-data:/var/lib/postgresql/data:rw
      - ./database/schemas:/docker-entrypoint-initdb.d:ro
      - ./database/migrations:/migrations:ro
      - postgres-logs:/var/log/postgresql:rw
    networks:
      - backend
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${DB_USER:-dharma_prod} -d ${DB_NAME:-iac_dharma_prod}"]
      <<: *common-health
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 4G
        reservations:
          cpus: '0.5'
          memory: 1G
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Redis Cache
  redis:
    image: redis:7-alpine
    container_name: dharma-redis-prod
    restart: unless-stopped
    <<: *common-security
    command: >
      redis-server
      --requirepass ${REDIS_PASSWORD:?Redis password required}
      --maxmemory 2gb
      --maxmemory-policy allkeys-lru
      --appendonly yes
      --appendfsync everysec
      --tcp-backlog 511
      --timeout 300
      --tcp-keepalive 300
    ports:
      - "127.0.0.1:6379:6379"
    volumes:
      - redis-data:/data:rw
      - redis-logs:/var/log/redis:rw
    networks:
      - backend
    healthcheck:
      test: ["CMD", "redis-cli", "--raw", "incr", "ping"]
      <<: *common-health
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 2G
        reservations:
          cpus: '0.25'
          memory: 512M
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ═══════════════════════════════════════════════════════════
  # APPLICATION TIER
  # ═══════════════════════════════════════════════════════════
  api-gateway:
    image: ${DOCKER_REGISTRY}/iac-dharma/api-gateway:${VERSION:-latest}
    container_name: dharma-api-gateway-prod
    restart: unless-stopped
    <<: *common-security
    environment:
      <<: *common-env
      PORT: 3001
      DB_HOST: postgres
      REDIS_HOST: redis
      JWT_SECRET: ${JWT_SECRET:?JWT secret required}
      ALLOWED_ORIGINS: ${ALLOWED_ORIGINS:?Allowed origins required}
      RATE_LIMIT_MAX_REQUESTS: 100
      RATE_LIMIT_WINDOW_MS: 900000
      WORKERS: ${WORKERS:-4}
      NODE_OPTIONS: --max-old-space-size=2048
      CLUSTER_MODE: "true"
    ports:
      - "3001:3001"
      - "127.0.0.1:9090:9090" # Metrics
    volumes:
      - api-logs:/var/log/api-gateway:rw
      - api-uploads:/app/uploads:rw
      - ./backend/cmdb-agent:/cmdb-agent:ro
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - backend
      - frontend
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:3001/api/health"]
      <<: *common-health
    deploy:
      replicas: ${API_REPLICAS:-2}
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 512M
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
        window: 120s
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "5"

  # Frontend
  frontend:
    image: ${DOCKER_REGISTRY}/iac-dharma/frontend:${VERSION:-latest}
    container_name: dharma-frontend-prod
    restart: unless-stopped
    <<: *common-security
    environment:
      VITE_API_URL: ${VITE_API_URL:?API URL required}
      VITE_WS_URL: ${VITE_WS_URL}
      VITE_APP_VERSION: ${VERSION:-1.0.0}
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./frontend/nginx-prod.conf:/etc/nginx/nginx.conf:ro
      - frontend-logs:/var/log/nginx:rw
      - ssl-certs:/etc/nginx/ssl:ro
    depends_on:
      - api-gateway
    networks:
      - frontend
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost/health"]
      <<: *common-health
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 128M
    logging:
      driver: "json-file"
      options:
        max-size: "20m"
        max-file: "5"

  # ═══════════════════════════════════════════════════════════
  # MONITORING TIER
  # ═══════════════════════════════════════════════════════════
  prometheus:
    image: prom/prometheus:latest
    container_name: dharma-prometheus
    restart: unless-stopped
    user: "65534:65534"
    read_only: true
    volumes:
      - ./monitoring/prometheus-prod.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus-data:/prometheus:rw
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=30d'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
    ports:
      - "127.0.0.1:9091:9090"
    networks:
      - monitoring
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:9090/-/healthy"]
      <<: *common-health
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 2G
        reservations:
          cpus: '0.25'
          memory: 512M

  grafana:
    image: grafana/grafana:latest
    container_name: dharma-grafana
    restart: unless-stopped
    user: "472:472"
    environment:
      GF_SECURITY_ADMIN_USER: ${GRAFANA_USER:-admin}
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_PASSWORD:?Grafana password required}
      GF_SERVER_ROOT_URL: ${GRAFANA_URL:-http://localhost:3002}
      GF_INSTALL_PLUGINS: grafana-clock-panel,grafana-piechart-panel
    volumes:
      - grafana-data:/var/lib/grafana:rw
      - ./monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards:ro
      - ./monitoring/grafana/datasources:/etc/grafana/provisioning/datasources:ro
    ports:
      - "127.0.0.1:3002:3000"
    depends_on:
      - prometheus
    networks:
      - monitoring
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:3000/api/health"]
      <<: *common-health
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M

  # Jaeger for distributed tracing
  jaeger:
    image: jaegertracing/all-in-one:latest
    container_name: dharma-jaeger
    restart: unless-stopped
    environment:
      COLLECTOR_ZIPKIN_HOST_PORT: :9411
      COLLECTOR_OTLP_ENABLED: true
    ports:
      - "127.0.0.1:16686:16686" # UI
      - "127.0.0.1:14268:14268" # Accept jaeger.thrift
    volumes:
      - jaeger-data:/tmp:rw
    networks:
      - monitoring
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 1G

# ═══════════════════════════════════════════════════════════
# NETWORKS
# ═══════════════════════════════════════════════════════════
networks:
  frontend:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/24
  backend:
    driver: bridge
    internal: true
    ipam:
      config:
        - subnet: 172.21.0.0/24
  monitoring:
    driver: bridge
    internal: true
    ipam:
      config:
        - subnet: 172.22.0.0/24

# ═══════════════════════════════════════════════════════════
# VOLUMES
# ═══════════════════════════════════════════════════════════
volumes:
  postgres-data:
    driver: local
  postgres-logs:
    driver: local
  redis-data:
    driver: local
  redis-logs:
    driver: local
  api-logs:
    driver: local
  api-uploads:
    driver: local
  frontend-logs:
    driver: local
  ssl-certs:
    driver: local
  prometheus-data:
    driver: local
  grafana-data:
    driver: local
  jaeger-data:
    driver: local
